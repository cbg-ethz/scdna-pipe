import glob
import os
import subprocess
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm
from pathlib import Path
from secondary_analysis import SecondaryAnalysis
sns.set()


fastqs_path = config['fastqs_path']
moved_fastqs_path = os.path.join(fastqs_path, "merged", "tricked")
analysis_path = config['analysis_path']

symlinks_path = os.path.join(analysis_path, '..', 'symlinks')
sym_raw_path = os.path.join(symlinks_path, "raw")
cellranger_path = os.path.join(analysis_path, "cellranger")
scripts_dir = config['scripts_dir']+'/'
raw_fastqs = glob.glob(fastqs_path + "*.fastq.gz")
fastq_lanes = ["L001","L002","L003","L004"]
sample_name = config['sample_name']
cr_sample_name = sample_name[:-3] # e.g. MHELAVELA_S2 becomes MHELAVELA
h5_path = config['secondary_analysis']['h5_path']
genes_path = config['secondary_analysis']['genes_path']
all_genes_path = config['secondary_analysis']['all_genes_path']

analysis_prefix = config['analysis_prefix']
seq_prefix = config["sequencing_prefix"]

sa = SecondaryAnalysis(
    sample_name=analysis_prefix,
    output_path=analysis_path,
    h5_path=h5_path,
    genes_path=genes_path,
    all_genes_path=all_genes_path,
)

def rename_fastq(s_name):
    '''
        renames the merged fastqs according to the bcl2fastq naming convention
        Sample input name: MERGED_BSSE_QGF_123456_ZXVN2SHG5_1_QWEERTY_T_scD_250c_r1v1_0_SI-GA-H5_S1_L003_I1_001.fastq.gz
    '''
    split_name = s_name.split('_')
    new_name = '_'.join(split_name[6:7]+split_name[-4:])
    return new_name


rule all:
    input:
        merged_fastqs = "merge_files_done.txt",
        rename_fastqs = "rename_fastqs_done.txt",
        tricked_fastqs = expand(fastqs_path+"/merged/tricked/" + sample_name + "_" + "{lane_no}" + "_R1_001.fastq.gz", lane_no = fastq_lanes),
        move_after_tricking_fastqs = "move_fastqs_to_tricked_done.txt",
        cellranger = "cellranger_done.txt",
        bins_genome = os.path.join(analysis_path, "filtering", analysis_prefix ) + "__bins_genome.tsv",
        chr_stops = os.path.join(analysis_path, "filtering", analysis_prefix ) + "__chr_stops.tsv",
        filtered_cnvs = os.path.join(analysis_path, "filtering", analysis_prefix ) + "__filtered_cnvs.tsv",
        filtered_counts = os.path.join(analysis_path, "filtering", analysis_prefix ) + "__filtered_counts.csv",
        filtered_counts_shape = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts_shape.txt",

        segmented_regions = os.path.join(analysis_path, "breakpoint_detection", analysis_prefix) + "_segmented_regions.txt",
        segmented_region_sizes = os.path.join(analysis_path, "breakpoint_detection", analysis_prefix) + "_segmented_region_sizes.txt",

        normalised_bins_heatmap = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_normalised_bins.png",
        normalised_bins_clustered_heatmap = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_normalised_bins_clustered.png",
        normalised_bins_clustered_bps_heatmap = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_normalised_bins_clustered_bps.png",

        segmented_counts = os.path.join(analysis_path,\
        "breakpoint_detection", analysis_prefix) + "_segmented_counts.csv",

        normalised_bins = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_bins.csv",
        normalised_regions = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_regions.csv",

        clustering_score = os.path.join(analysis_path, "clustering", analysis_prefix) + "__clustering_score.txt",
        phenograph_distance = os.path.join(analysis_path, "clustering", analysis_prefix) + "__phenograph_distance.csv",
        clusters_phenograph_assignment = os.path.join(analysis_path, "clustering", analysis_prefix) + "__clusters_phenograph_assignment.tsv"

        # cluster_frequencies = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cluster_frequencies.txt",
        # clustering_score = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__clustering_score.txt",
        # cluster_profile_overlapping = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cluster_profile_overlapping.png",
        # cluster_sizes = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cluster_sizes.txt",
        # clusters_phenograph_assignment = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__clusters_phenograph_assignment.tsv",
        # clusters_phenograph_cn_profiles = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__clusters_phenograph_cn_profiles.tsv",
        # clusters_phenograph_count_profiles = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__clusters_phenograph_count_profiles.tsv",
        # cn_cluster = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cn_cluster.h5",
        # cn_gene_cluster = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cn_gene_cluster.tsv",
        # tsne_output = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__tsne_output.png",
        # secondary_analysis = "secondary_analysis_done.txt",
        # cluster_plots_list = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cluster_profile_files.txt",
        # heatmap_plots_list = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cn_genes_clusters_files.txt",

        # symlink directories
        # raw_files_list = os.path.join(sym_raw_path, seq_prefix) + "__raw_files.txt",
        # sym_r1_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R1_001.fastq.gz", lane_no = fastq_lanes),
        # sym_r2_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R2_001.fastq.gz", lane_no = fastq_lanes),
        # sym_i1_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_I1_001.fastq.gz", lane_no = fastq_lanes),
        # checksum_file = os.path.join(sym_raw_path, seq_prefix) + "__raw_files.md5",

        # # copy cellranger outputs
        # cr_alarms_summary = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__alarms_summary.txt", 
        # cr_cnv_data = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__cnv_data.h5", 
        # cr_summary = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__summary.csv", 
        # cr_web_summary = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__web_summary.html"

    output:
        
    run:
        print("echo rule all")

rule remove_tenx_artifacts:
    params:
        bins = config["secondary_analysis"]["bins_to_remove"]
    input:
        os.path.join(cellranger_path, cr_sample_name) + "/outs/cnv_data.h5"
    output:
        bins_genome = os.path.join(analysis_path, "filtering", analysis_prefix) + "__bins_genome.tsv",
        chr_stops = os.path.join(analysis_path, "filtering", analysis_prefix) + "__chr_stops.tsv",
        filtered_cnvs = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_cnvs.csv",
        filtered_counts = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts.csv",
        filtered_counts_shape = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts_shape.txt"
    run:
        for key in config['secondary_analysis']:
            assert(os.path.isfile(config['secondary_analysis'][key]))
        sa.remove_tenx_genomics_artifacts(bins=params.bins)

rule detect_breakpoints:
    params:
        binary = config["breakpoint_detection"]["bin"],
        window_size = config["breakpoint_detection"]["window_size"],
        verbosity = config["breakpoint_detection"]["verbosity"],
        threshold = config["breakpoint_detection"]["threshold"],
        bp_detection_path = os.path.join(analysis_path, "breakpoint_detection"),
        posfix = analysis_prefix 
    input:
        d_matrix_file = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts.csv",
        matrix_shape = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts_shape.txt"
    output:
        segmented_regions = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_segmented_regions.txt",
        segmented_region_sizes = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_segmented_region_sizes.txt"
    run:
        try:
            os.makedirs(params.bp_detection_path)
        except FileExistsError:
            print("breakpoint detection directory already exists.")
        input_shape = np.loadtxt(input.matrix_shape)
        (n_cells, n_bins) = [int(input_shape[i]) for i in range(2)]

        try:
            cmd_output = subprocess.run([params.binary, f"--d_matrix_file={input.d_matrix_file}", f"--n_bins={n_bins}",\
                f"--n_cells={n_cells}", f"--window_size={params.window_size}", f"--postfix={params.posfix}",\
                f"--verbosity={params.verbosity}", f"--threshold={params.threshold}"])
        except subprocess.SubprocessError as e:
            print("Status : FAIL", e.returncode, e.output, e.stdout, e.stderr)
        else:
            print(f"subprocess out: {cmd_output}")
            print(f"stdout: {cmd_output.stdout}\n stderr: {cmd_output.stderr}")
        
        os.rename(params.posfix+"_segmented_regions.txt", output.segmented_regions)
        os.rename(params.posfix+"_segmented_region_sizes.txt", output.segmented_region_sizes)

rule plot_breakpoints:
    input:
        normalised_bins = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_bins.csv",
        normalised_regions = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_regions.csv",
        segmented_regions = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_segmented_regions.txt"
    output:
        normalised_bins_heatmap = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_normalised_bins.png",
        normalised_bins_clustered_heatmap = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_normalised_bins_clustered.png",
        normalised_bins_clustered_bps_heatmap = os.path.join(analysis_path,\
             "breakpoint_detection", analysis_prefix) + "_normalised_bins_clustered_bps.png"
    benchmark:
        "benchmark/plot_breakpoints.tsv"
    run:
        from scipy.cluster.hierarchy import ward, leaves_list
        from scipy.spatial.distance import pdist

        print("loading the normalised bins...")
        normalised_bins = np.loadtxt(input.normalised_bins, delimiter=',')
        print(f"shape of normalised bins: {normalised_bins.shape}")

        print("loading the normalised regions...")
        normalised_regions = np.loadtxt(input.normalised_regions, delimiter=',')
        print(f"shape of normalised regions: {normalised_regions.shape}")

        bps = np.loadtxt(input.segmented_regions)
        print(f"number of breakpoints: {len(bps)}")

        mean_normalised_bins = normalised_bins.mean()
        mean_normalised_regions = normalised_regions.mean()

        Z = ward(pdist(normalised_regions))
        hclust_index = leaves_list(Z)

        print("plotting the heatmap ...")
        plt.figure(figsize=(24, 8))
        ax = sns.heatmap(normalised_bins, vmax=mean_normalised_bins)
        plt.savefig(output.normalised_bins_heatmap)
        plt.close()

        print("plotting the clustered heatmap ...")
        plt.figure(figsize=(24, 8))
        ax = sns.heatmap(normalised_bins[hclust_index], vmax=mean_normalised_bins)
        plt.savefig(output.normalised_bins_clustered_heatmap)
        plt.close()

        print("plotting the clustered heatmap with breakpoints ...")
        plt.figure(figsize=(24, 8))
        ax = sns.heatmap(normalised_bins[hclust_index], vmax=mean_normalised_bins)
        ax = ax.vlines(bps, *ax.get_xlim(), colors='b', linestyles='dashed')
        plt.savefig(output.normalised_bins_clustered_bps_heatmap)
        plt.close()


rule segment_regions:
    input:
        filtered_counts = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts.csv",
        segmented_region_sizes = os.path.join(analysis_path,\
                "breakpoint_detection", analysis_prefix) + "_segmented_region_sizes.txt"
    output:
        segmented_counts = os.path.join(analysis_path,\
                "breakpoint_detection", analysis_prefix) + "_segmented_counts.csv"
    benchmark:
        "benchmark/segment_regions.tsv"
    run:
        print("loading the filtered counts...")
        filtered_counts = np.loadtxt(input.filtered_counts, delimiter=',')
        n_cells = filtered_counts.shape[0]
        region_sizes = np.loadtxt(input.segmented_region_sizes)
        n_regions = len(region_sizes)
        sum_region_sizes = np.sum(region_sizes)
        condensed_mat = np.zeros((n_cells, n_regions))

        print("segmenting the bins...")
        for i in tqdm(range(n_cells)):
            region_id = 0
            region_count = 0
            # import ipdb; ipdb.set_trace() # debugging starts here
            for j in range(int(sum_region_sizes)):
                to_add = filtered_counts[i][j]
                condensed_mat[i][region_id] += to_add
                region_count += 1
                if region_count == region_sizes[region_id]:
                    region_id += 1
                    region_count = 0

        if not np.allclose(condensed_mat.sum(axis=1), filtered_counts.sum(axis=1)):
            raise AssertionError(
                "not all values of the sums before & after "
                "segmentation are close")

        print("saving the segmented regions...")
        np.savetxt(
            output.segmented_counts,
            condensed_mat,
            delimiter=",",
        )

rule normalise_counts:
    input:
        filtered_counts = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts.csv",
        segmented_counts = os.path.join(analysis_path,\
                "breakpoint_detection", analysis_prefix) + "_segmented_counts.csv"
    output:
        normalised_bins = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_bins.csv",
        normalised_regions = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_regions.csv"
    benchmark:
        "benchmark/normalise_counts.tsv"    
    run:
        from sklearn.preprocessing import normalize

        print("loading the filtered counts...")
        filtered_counts = np.loadtxt(input.filtered_counts, delimiter=',')
        print("normalising the bins...")
        normalized_filtered_bins = normalize(filtered_counts, axis=1, norm="l1")
        # normalise but make the row sum equal to n_bins
        normalized_filtered_bins *= normalized_filtered_bins.shape[1]
        print(f"shape of normalised bins: {normalized_filtered_bins.shape}")
        print("saving the normalised bins...")
        np.savetxt(
            output.normalised_bins,
            normalized_filtered_bins,
            delimiter=",",
        )
        
        print("loading the segmented counts...")
        segmented_counts = np.loadtxt(input.segmented_counts, delimiter=',')
        print("normalising the regions...")
        normalized_filtered_regions = normalize(segmented_counts, axis=1, norm="l1")
        print(f"shape of normalised regions: {normalized_filtered_regions.shape}")
        print("saving the normalised bins...")
        np.savetxt(
            output.normalised_regions,
            normalized_filtered_regions,
            delimiter=",",
        )

rule clustering:
    input:
        normalised_regions = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_regions.csv"
    output:
        clustering_score = os.path.join(analysis_path, "clustering", analysis_prefix) + "__clustering_score.txt",
        phenograph_distance = os.path.join(analysis_path, "clustering", analysis_prefix) + "__phenograph_distance.csv",
        clusters_phenograph_assignment = os.path.join(analysis_path, "clustering", analysis_prefix) + "__clusters_phenograph_assignment.tsv"
    benchmark:
        "benchmark/clustering.tsv"
    run:
        sa.apply_phenograph(input.normalised_regions)

rule merge_files:
    params:
        fastqs_path = fastqs_path,
	scripts_dir = scripts_dir
    input:
        raw_fastqs = expand('{sample}', sample=raw_fastqs)
    output:
        done = "merge_files_done.txt"
    shell:
        "sh {params.scripts_dir}/merge_10x_gzip_files.sh {params.fastqs_path}; \
	if [ -d {params.fastqs_path}/merged ] ; \
	then \
	    echo merged directory exists;\
	else \
	    mkdir {params.fastqs_path}/merged;\
	fi ; \
        mv {params.fastqs_path}/MERGED_BSSE* {params.fastqs_path}/merged;\
        chmod 755 {params.fastqs_path}/merged/*;\
	touch merge_files_done.txt"

rule rename_fastqs:
    input:
        rules.merge_files.output.done
    output:
        "rename_fastqs_done.txt"
    run:
        merged_fastqs_path = fastqs_path + "/merged/"
	print(merged_fastqs_path)
	fastqs_dir = merged_fastqs_path
	for filename in os.listdir(fastqs_dir):
    	    if filename.startswith("MERGED_BSSE") and filename.endswith('.gz'):
    	        print("old name: " + filename)
                print("new name: " + rename_fastq(filename))
                os.rename(fastqs_dir+filename, fastqs_dir+rename_fastq(filename))
        Path('rename_fastqs_done.txt').touch()

rule trick_fastqs:
    params:
        fastqs_path = fastqs_path,
        scripts_dir = scripts_dir,
        r1 = fastqs_path+"/merged/" + sample_name + "_" + "{lane_no}" + "_R1_001.fastq.gz",
        r2 = fastqs_path+"/merged/" + sample_name + "_" + "{lane_no}" + "_R2_001.fastq.gz",
        mem = config["tricking_fastqs"]["mem"],
        time = config["tricking_fastqs"]["time"] 
    input:
        rules.rename_fastqs.output
    output:
        r1_fastqs = os.path.join(moved_fastqs_path, sample_name) + "_" + "{lane_no}" + "_R1_001.fastq.gz"
    shell:
        "\
        if [ -d {params.fastqs_path}/merged/tricked ] ; \
        then \
        echo tricked directory exists;\
        else \
        mkdir {params.fastqs_path}/merged/tricked;\
        fi ;\
        python {params.scripts_dir}/cellranger_dna_trick.py -r1 {params.r1}  -r2 {params.r2} -o {params.fastqs_path}/merged/tricked/"

rule move_fastqs:
    params:
        fastqs_path = fastqs_path,
        sample_name = sample_name
    input:
        tricked_fastqs = expand(os.path.join(moved_fastqs_path, sample_name) + "_" + "{lane_no}" + "_R1_001.fastq.gz", lane_no = fastq_lanes)
    output:
        move_after_tricking_fastqs = "move_fastqs_to_tricked_done.txt"
    shell:
        "mv {params.fastqs_path}/merged/*_R2_* {params.fastqs_path}/merged/tricked/;\
             mv {params.fastqs_path}/merged/*_I1_* {params.fastqs_path}/merged/tricked/;\
                  chmod 755 {params.fastqs_path}/merged/tricked/*;\
                      touch move_fastqs_to_tricked_done.txt;"

rule create_raw_files_list:
    params:
        sym_raw_path = sym_raw_path,
        seq_prefix = seq_prefix
    input:
        sym_r1_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R1_001.fastq.gz", lane_no = fastq_lanes),
        sym_r2_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R2_001.fastq.gz", lane_no = fastq_lanes),
        sym_i1_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_I1_001.fastq.gz", lane_no = fastq_lanes)
    output:
        os.path.join(sym_raw_path, seq_prefix) + "__raw_files.txt"  
    shell:
        "cd {params.sym_raw_path}; ls > {params.seq_prefix}__raw_files.txt "

rule create_raw_checksum:
    params:
        sym_raw_path = sym_raw_path,
        seq_prefix = seq_prefix
    input:
        os.path.join(sym_raw_path, seq_prefix) + "__raw_files.txt"
    output:
        checksum_file = os.path.join(sym_raw_path, seq_prefix) + "__raw_files.md5"
    shell:
        "cd {params.sym_raw_path}; find . -exec md5sum '{{}}' \; >  {output.checksum_file}"

rule create_cluster_plots_list:
    params:
        analysis_prefix = analysis_prefix,
        analysis_path = analysis_path
    input:
        secondary_analysis_done = "secondary_analysis_done.txt"
    output:
        cluster_plots_list = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cluster_profile_files.txt"
    shell:
        "cd {params.analysis_path}/clustering; ls | egrep '{params.analysis_prefix}__cluster_profile_..?\.png' > {params.analysis_prefix}__cluster_profile_files.txt"

rule create_heatmap_plots_list:
    params:
        analysis_prefix = analysis_prefix,
        analysis_path = analysis_path
    input:
        secondary_analysis_done = "secondary_analysis_done.txt"
    output:
        heatmap_plots_list = os.path.join(analysis_path, "clustering", analysis_prefix ) + "__cn_genes_clusters_files.txt"
    shell:
        "cd {params.analysis_path}/clustering; ls | egrep '{params.analysis_prefix}__cn_genes_clusters_chr..?_heatmap\.png' > {params.analysis_prefix}__cn_genes_clusters_files.txt"

rule run_cellranger:
    params:
        fastqs_path = fastqs_path+'/merged/tricked',
        cr_sample_name = cr_sample_name,
        cellranger_path = cellranger_path,
        local_cores = config['cellranger_dna']['local_cores'],
        local_mem = config['cellranger_dna']['local_mem'],
        mem_per_core = config['cellranger_dna']['mem_per_core'],
        mem = config['cellranger_dna']['mem'],
        time = config['cellranger_dna']['time']
    input:
        move_after_tricking_fastqs = "move_fastqs_to_tricked_done.txt",
        reference_path = config['ref_genome_path']
    output:
        cnv_data = os.path.join(cellranger_path, cr_sample_name) + "/outs/cnv_data.h5",
        cellranger_done = "cellranger_done.txt"
    shell:
        'if [ -d {params.cellranger_path}/run ] ; \
        then \
        echo cellranger directory exists;\
        else \
        mkdir {params.cellranger_path}/run;\
        fi ;\
         pushd {params.cellranger_path}/run; cellranger-dna cnv --reference={input.reference_path} --fastqs={params.fastqs_path}\
         --localmem={params.local_mem} --localcores={params.local_cores} --mempercore={params.mem_per_core}\
         --id={params.cr_sample_name} --sample={params.cr_sample_name}; ln -s "{params.cellranger_path}/run/{params.cr_sample_name}/outs/cnv_data.h5"\
         "{params.cellranger_path}/{params.cr_sample_name}/outs/cnv_data.h5"; popd; touch cellranger_done.txt'

rule copy_cellranger_outputs:
    params:
        cr_sample_name = cr_sample_name,
        cellranger_path = cellranger_path,
        analysis_prefix = analysis_prefix
    input:
        cellranger_done = "cellranger_done.txt"
    output:
        os.path.join(cellranger_path, "renamed", analysis_prefix) + "__alarms_summary.txt", 
        os.path.join(cellranger_path, "renamed", analysis_prefix) + "__cnv_data.h5", 
        os.path.join(cellranger_path, "renamed", analysis_prefix) + "__summary.csv", 
        os.path.join(cellranger_path, "renamed", analysis_prefix) + "__web_summary.html"
    shell:
        "cd {params.cellranger_path}; \
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/cnv_data.h5'\
             '{params.cellranger_path}/renamed/{params.analysis_prefix}__cnv_data.h5';\
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/alarms_summary.txt'\
             '{params.cellranger_path}/renamed/{params.analysis_prefix}__alarms_summary.txt';\
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/summary.csv'\
             '{params.cellranger_path}/renamed/{params.analysis_prefix}__summary.csv';\
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/web_summary.html'\
             '{params.cellranger_path}/renamed/{params.analysis_prefix}__web_summary.html';"

# rule secondary_analysis:
#     params:
#         analysis_prefix = analysis_prefix,
#         analysis_path = analysis_path,
#         h5_path = config['secondary_analysis']['h5_path'],
#         genes_path = config['secondary_analysis']['genes_path'],
#         all_genes_path = config['secondary_analysis']['all_genes_path'],
#         bins = config['secondary_analysis']['bins_to_remove'],
#         script_dir = scripts_dir+"../pipeline/run_secondary_analysis.py",
#         config_file = config['self']
#     input:
#         rules.run_cellranger.output.cnv_data
#     output:
#         bins_genome = os.path.join(analysis_path, "filtering", analysis_prefix) + "__bins_genome.tsv",
#         chr_stops = os.path.join(analysis_path, "filtering", analysis_prefix) + "__chr_stops.tsv",
#         filtered_cnvs = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_cnvs.tsv",
#         filtered_counts = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts.tsv",

#         cluster_frequencies = os.path.join(analysis_path, "clustering", analysis_prefix) + "__cluster_frequencies.txt",
#         clustering_score = os.path.join(analysis_path, "clustering", analysis_prefix) + "__clustering_score.txt",
#         cluster_profile_overlapping = os.path.join(analysis_path, "clustering", analysis_prefix) + "__cluster_profile_overlapping.png",
#         cluster_sizes = os.path.join(analysis_path, "clustering", analysis_prefix) + "__cluster_sizes.txt",
#         clusters_phenograph_assignment = os.path.join(analysis_path, "clustering", analysis_prefix) + "__clusters_phenograph_assignment.tsv",
#         clusters_phenograph_cn_profiles = os.path.join(analysis_path, "clustering", analysis_prefix) + "__clusters_phenograph_cn_profiles.tsv",
#         clusters_phenograph_count_profiles = os.path.join(analysis_path, "clustering", analysis_prefix) + "__clusters_phenograph_count_profiles.tsv",
#         cn_cluster = os.path.join(analysis_path, "clustering", analysis_prefix) + "__cn_cluster.h5",
#         cn_gene_cluster = os.path.join(analysis_path, "clustering", analysis_prefix) + "__cn_gene_cluster.tsv",
#         tsne_output = os.path.join(analysis_path, "clustering", analysis_prefix) + "__tsne_output.png",
#         done = "secondary_analysis_done.txt"
#     shell:
#         'python {params.script_dir} -c {params.config_file}; touch secondary_analysis_done.txt'

rule create_raw_symlinks:
    params:
        seq_prefix = seq_prefix,
        sym_raw_path = sym_raw_path,
        moved_fastqs_path = moved_fastqs_path,
        sample_name = sample_name,
        old_file_name = os.path.join(moved_fastqs_path, sample_name ) + "_"
    input:
        cellranger = "cellranger_done.txt",
        r1_fastqs = os.path.join(moved_fastqs_path, sample_name ) + "_" + "{lane_no}" + "_R1_001.fastq.gz",
        move_after_tricking_fastqs = "move_fastqs_to_tricked_done.txt"
        # r2_fastqs = os.path.join(moved_fastqs_path, sample_name ) + "_" + "{lane_no}" + "_R2_001.fastq.gz",
        # i1_fastqs = os.path.join(moved_fastqs_path, sample_name ) + "_" + "{lane_no}" + "_I1_001.fastq.gz"
    output:
        r1_fastqs = os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R1_001.fastq.gz",
        r2_fastqs = os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R2_001.fastq.gz",
        i1_fastqs = os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_I1_001.fastq.gz"
    shell:
        "ln -s {input.r1_fastqs} {output.r1_fastqs};\
            ln -s {params.old_file_name}{wildcards.lane_no}_R2_001.fastq.gz {output.r2_fastqs};\
                ln -s {params.old_file_name}{wildcards.lane_no}_I1_001.fastq.gz {output.i1_fastqs};"
