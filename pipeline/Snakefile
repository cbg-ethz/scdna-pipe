from scgenpy import *
from scgenpy.preprocessing.utils import *

from scicone import SCICoNE, Tree
from scicone.utils import *

import glob
import os
import h5py
import subprocess
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import phenograph
from tqdm import tqdm
from pathlib import Path
from collections import Counter
import re
import warnings
sns.set_style("ticks")

wildcard_constraints:
    stage = "((?!_)[A-Za-z])+",
    root = "((?!_)[A-Za-z])+"

scicone_path = config['scicone_path']
output_temp_path = config['output_temp_path']

bin_size = config['bin_size']
fastqs_path = config['fastqs_path']
moved_fastqs_path = os.path.join(fastqs_path, "merged", "tricked")
analysis_path = config['analysis_path']

analysis_prefix = config['analysis_prefix']
seq_prefix = config["sequencing_prefix"]

to_upload_path = os.path.join(analysis_path, '..', 'to_upload', analysis_prefix)
sym_raw_path = os.path.join(to_upload_path, "raw")
sym_derived_path = os.path.join(to_upload_path, "derived")
cellranger_path = os.path.join(analysis_path, "cellranger")
scripts_dir = config['scripts_dir']+'/'
raw_fastqs = glob.glob(fastqs_path + "*.fastq.gz")
n_lanes = config['n_lanes']
fastq_lanes = [f"L00{i}" for i in range(1, n_lanes+1)]
sample_name = config['sample_name']
cr_sample_name = sample_name[:-3] # e.g. MHELAVELA_S2 becomes MHELAVELA
h5_path = config['secondary_analysis']['h5_path']
gene_lists_path = config['secondary_analysis']['genes_path']
gene_coordinates_path = os.path.join(gene_lists_path, 'ensembl_hg19_annotations.tsv')
disease_genes_path = os.path.join(gene_lists_path, 'disease_specific', f"{config['disease']}_genes.txt")
general_main_gene_list_path = os.path.join(gene_lists_path, 'general', f"{config['secondary_analysis']['general_main_gene_list']}")

try:
    tree_rep = config["inference"]["cluster_trees"]["n_reps"]
except KeyError:
    tree_rep = 10

derived_file_names =[
    "cnv_data.h5",
    "chr_stops.tsv",
    "bins_genome.tsv",
    "filtered_counts.csv",
    "excluded_bins.csv",
    "segmented_regions.txt",
    "segmented_region_sizes.txt",
    "segmented_counts.csv",
    "normalised_bins.csv",
    "normalised_regions.csv",
    "clustering_score.txt",
    "cluster_tree.json",
    "cluster_tree.txt",
    "cluster_tree_sorted_normalised_counts_bins.png",
    "cluster_tree_sorted_cnvs_bins.png",
    "unique_cluster_tree_cnvs.csv",
    "inferred_cnvs.csv",
    "cluster_tree_genes.png",
    "cluster_profile_files.txt",
    "cluster_profile_overlapping.png",
    "cn_gene_df.csv",
    "cn_gene_df_roche_gene_list.csv",
    "heatmap_cnvs.png",
    "clone_lib_sizes.png",
    "Summary.txt"
]
run_lite = (config['lite'] == "True")
hotstart = (config['hotstart'] == "True")
print(f"Lite: {run_lite}.")
print(f"Hot start: {hotstart}.")

if not run_lite:
    derived_file_names.append("web_summary.html")
    derived_file_names.append("summary.csv")
    derived_file_names.append("alarms_summary.txt")
    derived_file_names.append("possorted_bam.bam")

tree_outputs = ["cluster_tree"]

sa = SecondaryAnalysis(
    sample_name=analysis_prefix,
    output_path=analysis_path,
    h5_path=h5_path
)

# import rules
include: os.path.join(workflow.basedir, "rules", "tree_learning.smk")
include: os.path.join(workflow.basedir, "rules", "breakpoint_detection.smk")
include: os.path.join(workflow.basedir, "rules", "process_cnvs.smk")
include: os.path.join(workflow.basedir, "rules", "plotting.smk")

onstart:
    print(f"Workflow main directory: {workflow.basedir}")


if not hotstart:
    rule all:
        input:
            raw_checksum_file = os.path.join(sym_raw_path, f"{seq_prefix}__raw_files.md5"),
            derived_checksum_file = os.path.join(sym_derived_path, f"{analysis_prefix}__derived_files.md5")
        run:
            print("rule all")
else:
    rule all:
        input:
            derived_checksum_file = os.path.join(sym_derived_path, f"{analysis_prefix}__derived_files.md5")
        run:
            print("rule all")

rule filter_counts:
    params:
        bins = config["secondary_analysis"]["bins_to_remove"],
        alpha = config["outlier_detection"]["alpha"],
        bin_threshold = config["outlier_detection"]["bin_threshold"]
    input:
        h5_file = h5_path
    output:
        bin_filtered_counts = os.path.join(analysis_path, "filtering", analysis_prefix) + "__bin_filtered_counts.csv",
        filtered_counts = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts.csv",
        filtered_counts_shape = os.path.join(analysis_path, "filtering", analysis_prefix) + "__filtered_counts_shape.txt",
        excluded_bins = os.path.join(analysis_path, "filtering", analysis_prefix) + "__excluded_bins.csv",
        is_outlier = os.path.join(analysis_path, "filtering", analysis_prefix) + "_is_outlier.txt"
    run:
        bin_filtered_counts, excluded_bins = sa.remove_tenx_genomics_artifacts(bins=params.bins, bin_threshold=params.bin_threshold, to_file=False)
        filtered_counts, is_outlier = sa.remove_outliers(bin_filtered_counts, alpha=params.alpha, median_thres=0)
        print(f"Marked {np.count_nonzero(is_outlier)}/{bin_filtered_counts.shape[0]} cells as outliers.")
        print(f"Final data shape: {filtered_counts.shape}")

        np.savetxt(output.is_outlier, is_outlier, delimiter=",")
        np.savetxt(output.excluded_bins, excluded_bins, delimiter=",")
        np.savetxt(output.filtered_counts_shape, filtered_counts.shape)
        np.savetxt(output.bin_filtered_counts, bin_filtered_counts, delimiter=",")
        np.savetxt(output.filtered_counts, filtered_counts, delimiter=",")

rule extract_genomic_info:
    params:
        gender = config['gender']
    input:
        h5_file = h5_path
    output:
        chr_stops = os.path.join(analysis_path, "genomic_coordinates", analysis_prefix) + "__chr_stops.tsv",
        bin_chr_indicator = os.path.join(analysis_path, "genomic_coordinates", analysis_prefix) + "__bin_chr_indicator.txt",
        bins_genome = os.path.join(analysis_path, "genomic_coordinates", analysis_prefix) + "__bins_genome.tsv"
    run:
        sa.extract_genomic_info(params.gender)

rule add_filtered_bins_back:
    input:
        cnv_profiles = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_inferred_cnvs_final_{root}.csv", # use cluster tree results
        unique_cnv_profiles = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__unique_cluster_tree_cnvs_final_{root}.csv", # use cluster tree results
        excluded_bins = os.path.join(analysis_path, "filtering", analysis_prefix) + "__excluded_bins.csv"
    output:
        inferred_cnvs = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__inferred_cnvs_final_{root}.csv",
        unique_cnvs = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__unique_cnvs_final_{root}.csv"
    run:
        sa.add_filtered_bins_back(input.cnv_profiles, input.excluded_bins, output.inferred_cnvs)
        sa.add_filtered_bins_back(input.unique_cnv_profiles, input.excluded_bins, output.unique_cnvs)

rule merge_files:
    params:
        fastqs_path = fastqs_path,
        n_lanes = n_lanes,
        scripts_dir = scripts_dir
    input:
        raw_fastqs = expand('{sample}', sample=raw_fastqs)
    output:
        done = "merge_files_done.txt"
    shell:
        "sh {params.scripts_dir}/merge_10x_gzip_files.sh {params.fastqs_path} {params.n_lanes}; \
    	if [ -d {params.fastqs_path}/merged ] ; \
    	then \
    	    echo merged directory exists;\
    	else \
    	    mkdir {params.fastqs_path}/merged;\
    	fi ; \
            mv {params.fastqs_path}/MERGED_BSSE* {params.fastqs_path}/merged;\
            chmod 775 {params.fastqs_path}/merged/*;\
    	touch merge_files_done.txt"

rule rename_fastqs:
    input:
        rules.merge_files.output.done
    output:
        "rename_fastqs_done.txt"
    run:
        merged_fastqs_path = fastqs_path + "/merged/"
        print(merged_fastqs_path)
        fastqs_dir = merged_fastqs_path
        for filename in os.listdir(fastqs_dir):
            if filename.startswith("MERGED_BSSE") and filename.endswith('.gz'):
                print("old name: " + filename)
                print("new name: " + rename_fastq(filename))
                os.rename(fastqs_dir+filename, fastqs_dir+rename_fastq(filename, sample_name=sample_name))
        Path('rename_fastqs_done.txt').touch()


rule trick_fastqs:
    params:
        fastqs_path = fastqs_path,
        scripts_dir = scripts_dir,
        r1 = fastqs_path+"/merged/" + sample_name + "_" + "{lane_no}" + "_R1_001.fastq.gz",
        r2 = fastqs_path+"/merged/" + sample_name + "_" + "{lane_no}" + "_R2_001.fastq.gz",
        mem = config["tricking_fastqs"]["mem"],
        time = config["tricking_fastqs"]["time"],
        insert_length = config["tricking_fastqs"]["insert_length"]
    input:
        rules.rename_fastqs.output
    output:
        r1_fastqs = os.path.join(moved_fastqs_path, sample_name) + "_" + "{lane_no}" + "_R1_001.fastq.gz"
    shell:
        "\
        if [ -d {params.fastqs_path}/merged/tricked ] ; \
        then \
        echo tricked directory exists;\
        else \
        mkdir {params.fastqs_path}/merged/tricked;\
        fi ;\
        python {params.scripts_dir}/cellranger_dna_trick.py -r1 {params.r1}  -r2 {params.r2} -l {params.insert_length} -o {params.fastqs_path}/merged/tricked/"

rule move_fastqs:
    params:
        fastqs_path = fastqs_path,
        sample_name = sample_name
    input:
        tricked_fastqs = expand(os.path.join(moved_fastqs_path, sample_name) + "_" + "{lane_no}" + "_R1_001.fastq.gz", lane_no = fastq_lanes)
    output:
        move_after_tricking_fastqs = "move_fastqs_to_tricked_done.txt"
    shell:
        "mv {params.fastqs_path}/merged/*_R2_* {params.fastqs_path}/merged/tricked/;\
             mv {params.fastqs_path}/merged/*_I1_* {params.fastqs_path}/merged/tricked/;\
                  chmod 755 {params.fastqs_path}/merged/tricked/*;\
                      touch move_fastqs_to_tricked_done.txt;"

rule create_raw_files_list:
    params:
        sym_raw_path = sym_raw_path,
        seq_prefix = seq_prefix
    input:
        sym_r1_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R1_001.fastq.gz", lane_no = fastq_lanes),
        sym_r2_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R2_001.fastq.gz", lane_no = fastq_lanes),
        sym_i1_fastqs = expand(os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_I1_001.fastq.gz", lane_no = fastq_lanes)
    output:
        os.path.join(sym_raw_path, seq_prefix) + "__raw_files.txt"
    shell:
        "cd {params.sym_raw_path}; ls > {params.seq_prefix}__raw_files.txt "

rule create_raw_checksum:
    params:
        sym_raw_path = sym_raw_path,
        seq_prefix = seq_prefix
    input:
        os.path.join(sym_raw_path, seq_prefix) + "__raw_files.txt"
    output:
        checksum_file = os.path.join(sym_raw_path, seq_prefix) + "__raw_files.md5"
    shell:
        "cd {params.sym_raw_path}; find . -exec md5sum '{{}}' \; >  {output.checksum_file}"

rule run_cellranger:
    params:
        fastqs_path = fastqs_path+'/merged/tricked',
        cr_sample_name = cr_sample_name,
        cellranger_path = cellranger_path,
        local_cores = config['cellranger_dna']['local_cores'],
        local_mem = config['cellranger_dna']['local_mem'],
        mem_per_core = config['cellranger_dna']['mem_per_core'],
        mem = config['cellranger_dna']['mem'],
        time = config['cellranger_dna']['time']
    input:
        move_after_tricking_fastqs = "move_fastqs_to_tricked_done.txt",
        reference_path = config['ref_genome_path']
    output:
        cnv_data = os.path.join(cellranger_path, cr_sample_name) + "/outs/cnv_data.h5",
        cellranger_done = "cellranger_done.txt"
    shell:
        'if [ -d {params.cellranger_path}/run ] ; \
        then \
        echo cellranger directory exists;\
        else \
        mkdir {params.cellranger_path}/run;\
        fi ;\
         pushd {params.cellranger_path}/run; cellranger-dna cnv --reference={input.reference_path} --fastqs={params.fastqs_path}\
         --localmem={params.local_mem} --localcores={params.local_cores} --mempercore={params.mem_per_core}\
         --id={params.cr_sample_name} --sample={params.cr_sample_name}; ln -s "{params.cellranger_path}/run/{params.cr_sample_name}/outs/cnv_data.h5"\
         "{params.cellranger_path}/{params.cr_sample_name}/outs/cnv_data.h5"; popd; touch cellranger_done.txt'

rule copy_cellranger_outputs:
    params:
        cr_sample_name = cr_sample_name,
        cellranger_path = cellranger_path,
        analysis_prefix = analysis_prefix
    input:
        cellranger_done = "cellranger_done.txt"
    output:
        alarms_summary = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__alarms_summary.txt",
        cnv_data = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__cnv_data.h5",
        summary = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__summary.csv",
        web_summary = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__web_summary.html",
        bam = os.path.join(cellranger_path, "renamed", analysis_prefix) + "__possorted_bam.bam",
    shell:
        "cd {params.cellranger_path}; \
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/alarms_summary.txt' '{output.alarms_summary}';\
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/cnv_data.h5' '{output.cnv_data}';\
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/summary.csv' '{output.summary}';\
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/web_summary.html' '{output.web_summary}';\
         ln -s '{params.cellranger_path}/run/{params.cr_sample_name}/outs/possorted_bam.bam' '{output.bam}';"

rule create_cluster_plots_list:
    params:
        analysis_prefix = analysis_prefix,
        analysis_path = analysis_path
    input:
        secondary_analysis_done = "secondary_analysis_done.txt",
    output:
        cluster_plots_list = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix ) + "__cluster_profile_files.txt"
    shell:
        "cd {params.analysis_path}/inferred_cnvs; ls | egrep '{params.analysis_prefix}__cluster_profile_..?\.png' > {params.analysis_prefix}__cluster_profile_files.txt"

rule create_heatmap_plots_list:
    params:
        analysis_prefix = analysis_prefix,
        analysis_path = analysis_path
    input:
        secondary_analysis_done = "secondary_analysis_done.txt"
    output:
        heatmap_plots_list = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix ) + "__cn_genes_clusters_files.txt"
    shell:
        "cd {params.analysis_path}/inferred_cnvs; ls | egrep '{params.analysis_prefix}__cn_genes_clusters_chr..?_heatmap\.png' > {params.analysis_prefix}__cn_genes_clusters_files.txt"

rule create_raw_symlinks:
    params:
        seq_prefix = seq_prefix,
        sym_raw_path = sym_raw_path,
        moved_fastqs_path = moved_fastqs_path,
        sample_name = sample_name,
        old_file_name = os.path.join(moved_fastqs_path, sample_name ) + "_"
    input:
        cellranger = "cellranger_done.txt",
        r1_fastqs = os.path.join(moved_fastqs_path, sample_name ) + "_" + "{lane_no}" + "_R1_001.fastq.gz",
        move_after_tricking_fastqs = "move_fastqs_to_tricked_done.txt"
        # r2_fastqs = os.path.join(moved_fastqs_path, sample_name ) + "_" + "{lane_no}" + "_R2_001.fastq.gz",
        # i1_fastqs = os.path.join(moved_fastqs_path, sample_name ) + "_" + "{lane_no}" + "_I1_001.fastq.gz"
    output:
        r1_fastqs = os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R1_001.fastq.gz",
        r2_fastqs = os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_R2_001.fastq.gz",
        i1_fastqs = os.path.join(sym_raw_path, seq_prefix) + "__" + "{lane_no}" + "_I1_001.fastq.gz"
    shell:
        "ln -s {input.r1_fastqs} {output.r1_fastqs};\
            ln -s {params.old_file_name}{wildcards.lane_no}_R2_001.fastq.gz {output.r2_fastqs};\
                ln -s {params.old_file_name}{wildcards.lane_no}_I1_001.fastq.gz {output.i1_fastqs};"

rule symlink_best_tree:
    input:
        clustering_score_diploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__clustering_score_final_diploid.txt",
        cluster_tree_json_diploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_final_diploid.json",
        cluster_tree_diploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_final_diploid.txt",
        cluster_tree_sorted_normalised_counts_bins_diploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_tree_sorted_normalised_counts_bins_diploid.png",
        cluster_tree_sorted_cnvs_bins_diploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_tree_sorted_cnvs_bins_diploid.png",
        unique_cnv_profiles_diploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__unique_cluster_tree_cnvs_final_diploid.csv",
        inferred_cnvs_diploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__inferred_cnvs_final_diploid.csv",
        cluster_tree_genes_png_diploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_genes_diploid.png",
        overlapping_cluster_plot_diploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_profile_overlapping_diploid.png",
        gene_cn_df_diploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cn_gene_df_diploid.csv",
        gene_cn_df_roche_gene_list_diploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cn_gene_df_diploid_roche_gene_list.csv",
        heatmap_cnvs_diploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__heatmap_cnvs_diploid.png",
        clone_lib_sizes_diploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__clone_lib_sizes_diploid.png",
        clustering_score_tetraploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__clustering_score_final_tetraploid.txt",
        cluster_tree_json_tetraploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_final_tetraploid.json",
        cluster_tree_tetraploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_final_tetraploid.txt",
        cluster_tree_sorted_normalised_counts_bins_tetraploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_tree_sorted_normalised_counts_bins_tetraploid.png",
        cluster_tree_sorted_cnvs_bins_tetraploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_tree_sorted_cnvs_bins_tetraploid.png",
        unique_cnv_profiles_tetraploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__unique_cluster_tree_cnvs_final_tetraploid.csv",
        inferred_cnvs_tetraploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__inferred_cnvs_final_tetraploid.csv",
        cluster_tree_genes_png_tetraploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_genes_tetraploid.png",
        overlapping_cluster_plot_tetraploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_profile_overlapping_tetraploid.png",
        gene_cn_df_tetraploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cn_gene_df_tetraploid.csv",
        gene_cn_df_roche_gene_list_tetraploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cn_gene_df_tetraploid_roche_gene_list.csv",
        heatmap_cnvs_tetraploid = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__heatmap_cnvs_tetraploid.png",
        clone_lib_sizes_tetraploid = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__clone_lib_sizes_tetraploid.png",
    output:
        clustering_score = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__clustering_score_final.txt",
        cluster_tree_json = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_final.json",
        cluster_tree = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_final.txt",
        cluster_tree_sorted_normalised_counts_bins = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_tree_sorted_normalised_counts_bins.png",
        cluster_tree_sorted_cnvs_bins = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_tree_sorted_cnvs_bins.png",
        unique_cnv_profiles = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__unique_cluster_tree_cnvs_final.csv",
        inferred_cnvs = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__inferred_cnvs_final.csv",
        cluster_tree_genes_png = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_genes.png",
        overlapping_cluster_plot = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_profile_overlapping.png",
        gene_cn_df = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cn_gene_df.csv",
        gene_cn_df_roche_gene_list = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cn_gene_df_roche_gene_list.csv",
        heatmap_cnvs = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__heatmap_cnvs.png",
        clone_lib_sizes = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__clone_lib_sizes.png",
        best_tree = os.path.join(analysis_path, "tree_learning", analysis_prefix) + '_best_tree.txt',
    run:
        def create_symlink(source, dest):
            if not os.path.isfile(dest):
                os.symlink(source, dest)
            else:
                os.utime(dest, None)  # Set access/modified times to now

        with open(input.cluster_tree_diploid, 'r') as f:
            diploid = f.readlines()
        diploid_score = float(diploid[5].split(' ')[-1].split('\n')[0])
        with open(input.cluster_tree_tetraploid, 'r') as f:
            tetraploid = f.readlines()
        tetraploid_score = float(tetraploid[5].split(' ')[-1].split('\n')[0])
        print("Tree scores:")
        print(f"- diploid:\t{diploid_score}")
        print(f"- tetraploid:\t{tetraploid_score}")
        with open(output.best_tree, 'w') as f:
            if diploid_score > tetraploid_score:
                f.write('diploid')
                print(f"Best: diploid")
                init = 0
            else:
                f.write('tetraploid')
                print(f"Best: tetraploid")
                init = len(output)-1

        for i in range(len(output)-1):
            create_symlink(input[init+i], output[i])

rule create_derived_symlinks:
    params:
        cellranger_path = cellranger_path,
        analysis_prefix = analysis_prefix,
        ploidy = config["inference"]["ploidy"]
    input:
        chr_stops = os.path.join(analysis_path, "genomic_coordinates", analysis_prefix) + "__chr_stops.tsv",
        bins_genome = os.path.join(analysis_path, "genomic_coordinates", analysis_prefix) + "__bins_genome.tsv",
        filtered_counts = os.path.join(analysis_path, "filtering", analysis_prefix ) + "__filtered_counts.csv",
        excluded_bins = os.path.join(analysis_path, "filtering", analysis_prefix) + "__excluded_bins.csv",
        segmented_regions = os.path.join(analysis_path, "breakpoint_detection", analysis_prefix) + "_segmented_final_regions.txt",
        segmented_region_sizes = os.path.join(analysis_path, "breakpoint_detection", analysis_prefix) + "_segmented_final_region_sizes.txt",
        segmented_counts = os.path.join(analysis_path, "breakpoint_detection", analysis_prefix) + "_segmented_final_counts.csv",
        normalised_bins = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_bins_final.csv",
        normalised_regions = os.path.join(analysis_path, "normalisation", analysis_prefix) + "__normalised_regions_final.csv",
        clustering_score = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__clustering_score_final.txt",
        cluster_tree_json = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_final.json",
        cluster_tree = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_final.txt",
        cluster_tree_sorted_normalised_counts_bins = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_tree_sorted_normalised_counts_bins.png",
        cluster_tree_sorted_cnvs_bins = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_tree_sorted_cnvs_bins.png",
        unique_cnv_profiles = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__unique_cluster_tree_cnvs_final.csv",
        inferred_cnvs = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__inferred_cnvs_final.csv",
        cluster_tree_genes_png = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__cluster_tree_genes.png",
        overlapping_cluster_plot = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cluster_profile_overlapping.png",
        gene_cn_df = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cn_gene_df.csv",
        gene_cn_df_roche_gene_list = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__cn_gene_df_roche_gene_list.csv",
        heatmap_cnvs = os.path.join(analysis_path, "inferred_cnvs", analysis_prefix) + "__heatmap_cnvs.png",
        clone_lib_sizes = os.path.join(analysis_path, "tree_learning", analysis_prefix) + "__clone_lib_sizes.png",
        drugs_responsive_genes = os.path.join(config['drugs_path'], "drugs_responsive_genes.txt"),
        drugs_resistant_genes = os.path.join(config['drugs_path'], "drugs_resistant_genes.txt"),
        best_tree = os.path.join(analysis_path, "tree_learning", analysis_prefix) + '_best_tree.txt',
    output:
        derived_symlinks = [os.path.join(sym_derived_path, f"{analysis_prefix}__{filename}") for filename in derived_file_names]
    run:
        def create_symlink(source, dest):
            if not os.path.isfile(dest):
                os.symlink(source, dest)
            else:
                os.utime(dest, None)  # Set access/modified times to now

        if not run_lite:
            create_symlink(os.path.join(cellranger_path, "renamed", analysis_prefix) + "__cnv_data.h5", os.path.join(sym_derived_path, f"{analysis_prefix}__cnv_data.h5"))
            create_symlink(os.path.join(cellranger_path, "renamed", analysis_prefix) + "__alarms_summary.txt", os.path.join(sym_derived_path, f"{analysis_prefix}__alarms_summary.txt"))
            create_symlink(os.path.join(cellranger_path, "renamed", analysis_prefix) + "__summary.csv", os.path.join(sym_derived_path, f"{analysis_prefix}__summary.csv"))
            create_symlink(os.path.join(cellranger_path, "renamed", analysis_prefix) + "__web_summary.html", os.path.join(sym_derived_path, f"{analysis_prefix}__web_summary.html"))
            create_symlink(os.path.join(cellranger_path, "renamed", analysis_prefix) + "__possorted_bam.bam", os.path.join(sym_derived_path, f"{analysis_prefix}__possorted_bam.bam"))
        else:
            create_symlink(h5_path, os.path.join(sym_derived_path, f"{analysis_prefix}__cnv_data.h5"))

        create_symlink(input.chr_stops, os.path.join(sym_derived_path, f"{analysis_prefix}__chr_stops.tsv"))
        create_symlink(input.bins_genome, os.path.join(sym_derived_path, f"{analysis_prefix}__bins_genome.tsv"))
        create_symlink(input.filtered_counts, os.path.join(sym_derived_path, f"{analysis_prefix}__filtered_counts.csv"))

        create_symlink(input.excluded_bins, os.path.join(sym_derived_path, f"{analysis_prefix}__excluded_bins.csv"))

        create_symlink(input.segmented_regions, os.path.join(sym_derived_path, f"{analysis_prefix}__segmented_regions.txt"))
        create_symlink(input.segmented_region_sizes, os.path.join(sym_derived_path, f"{analysis_prefix}__segmented_region_sizes.txt"))
        create_symlink(input.segmented_counts, os.path.join(sym_derived_path, f"{analysis_prefix}__segmented_counts.csv"))

        create_symlink(input.normalised_bins, os.path.join(sym_derived_path, f"{analysis_prefix}__normalised_bins.csv"))
        create_symlink(input.normalised_regions, os.path.join(sym_derived_path, f"{analysis_prefix}__normalised_regions.csv"))

        create_symlink(input.cluster_tree_sorted_normalised_counts_bins, os.path.join(sym_derived_path, f"{analysis_prefix}__cluster_tree_sorted_normalised_counts_bins.png"))
        create_symlink(input.cluster_tree_sorted_cnvs_bins, os.path.join(sym_derived_path, f"{analysis_prefix}__cluster_tree_sorted_cnvs_bins.png"))

        create_symlink(input.clustering_score, os.path.join(sym_derived_path, f"{analysis_prefix}__clustering_score.txt"))

        create_symlink(input.cluster_tree_json, os.path.join(sym_derived_path, f"{analysis_prefix}__cluster_tree.json"))
        create_symlink(input.cluster_tree, os.path.join(sym_derived_path, f"{analysis_prefix}__cluster_tree.txt"))
        create_symlink(input.unique_cnv_profiles, os.path.join(sym_derived_path, f"{analysis_prefix}__unique_cluster_tree_cnvs.csv"))
        create_symlink(input.inferred_cnvs, os.path.join(sym_derived_path, f"{analysis_prefix}__inferred_cnvs.csv"))
        create_symlink(input.cluster_tree_genes_png, os.path.join(sym_derived_path, f"{analysis_prefix}__cluster_tree_genes.png"))

        create_symlink(input.overlapping_cluster_plot, os.path.join(sym_derived_path, f"{analysis_prefix}__cluster_profile_overlapping.png"))
        create_symlink(input.clone_lib_sizes, os.path.join(sym_derived_path, f"{analysis_prefix}__clone_lib_sizes.png"))

        create_symlink(input.gene_cn_df, os.path.join(sym_derived_path, f"{analysis_prefix}__cn_gene_df.csv"))
        create_symlink(input.gene_cn_df_roche_gene_list, os.path.join(sym_derived_path, f"{analysis_prefix}__cn_gene_df_roche_gene_list.csv"))
        create_symlink(input.heatmap_cnvs, os.path.join(sym_derived_path, f"{analysis_prefix}__heatmap_cnvs.png"))

        tree_is_diploid = True
        ploidy = 'diploid'
        with open(input.best_tree, 'r') as f:
            lines = f.read()
            tree_is_diploid = (lines.split('\n')[0] == 'diploid')
        if not tree_is_diploid:
            ploidy = 'tetraploid'
        cluster_profile_plots = glob.glob(os.path.join(analysis_path, "inferred_cnvs", f"{analysis_prefix}__cluster_profile_[0-9]*_{ploidy}.png"))
        print(cluster_profile_plots)
        with open(os.path.join(sym_derived_path, f"{analysis_prefix}__cluster_profile_files.txt"), "w") as file:
            for cluster_profile_plot in cluster_profile_plots:
                f_name = cluster_profile_plot.split("/")[-1]
                create_symlink(cluster_profile_plot, os.path.join(sym_derived_path, f"{f_name}"))
                file.write(f"{f_name}\n")

        # write the summary txt
        cn_gene_df = pd.read_csv(input.gene_cn_df, index_col=0)
        if 'is_imputed' in cn_gene_df.columns:
            cn_gene_df = cn_gene_df.drop(columns=['is_imputed'])
        root_states = np.array(cn_gene_df['neutral_state']).astype(int)
        cn_gene_df = cn_gene_df.drop(columns=['neutral_state'])

        # Don't consider the root node cells (i.e. cells with copy number equal to root state across all assayed genes)
        try:
            neutral_cluster = np.where((cn_gene_df.values.T == root_states).all(1))[0][0]
            cn_gene_df = cn_gene_df.drop(columns=cn_gene_df.columns[neutral_cluster])
        except Exception as e:
            print(cn_gene_df)
            print(root_states)
            print(e)
        n_clusters = len(cn_gene_df.columns)

        segmented_counts = np.loadtxt(input.segmented_counts, delimiter=',')
        n_cells = segmented_counts.shape[0]

        if not tree_is_diploid:
            root_states = root_states*2
        root_states = root_states.reshape(-1,1)
        amplified_genes = cn_gene_df[(cn_gene_df > root_states).any(axis='columns')].index.values.tolist()
        n_amplified_genes = len(amplified_genes)
        amplified_genes_str = ', '.join(amplified_genes)
        deleted_genes = cn_gene_df[(cn_gene_df < root_states).any(axis='columns')].index.values.tolist()
        n_deleted_genes = len(deleted_genes)
        deleted_genes_str = ', '.join(deleted_genes)

        open(os.path.join(sym_derived_path, f"{analysis_prefix}__Summary.txt"), "w").close()

        with open(os.path.join(sym_derived_path, f"{analysis_prefix}__Summary.txt"), "w") as summary_file:
            warning_str = ""
            if n_cells  > 2 and n_cells < 20:
                warning_str = f"Warning: only {n_cells} cells detected. These results may only be used to confirm results from other technologies.\n"
            if n_cells <=2:
                warning_str = f"Only {n_cells} cell(s) detected. No analysis performed.\n"

            summary_file.write(warning_str)

            if n_cells > 2:
                if n_clusters > 1:
                    verb = 'are'
                    clones = 'clones'
                else:
                    verb = 'is'
                    clones = 'clone'
                if n_amplified_genes > 0:
                    amp_str = f"copy number gains in {amplified_genes_str}"
                else:
                    amp_str = f"no copy number gains"
                if n_deleted_genes > 0:
                    del_str = f"copy number deletions in {deleted_genes_str}"
                else:
                    del_str = f"no copy number deletions"

                summary_file.write(f"There {verb} {n_clusters} {clones} detected with {amp_str}"
                f" and {del_str} among the pre-selected genes, relative to the {ploidy} state.\n\n")

                all_genes = set()
                all_genes.update(set(amplified_genes))
                all_genes.update(set(deleted_genes))

                # Gene - drug associations.
                df = pd.read_csv(input.drugs_responsive_genes, header=None)
                drug_map = zip(df[0], df[1])
                drugs_responsive_genes = set()
                for item in drug_map:
                    drug = item[0]
                    gene = item[1]
                    if gene in all_genes:
                        drugs_responsive_genes.add(drug)
                drugs_responsive_genes = list(drugs_responsive_genes)
                drugs_responsive_genes.sort()

                df = pd.read_csv(input.drugs_resistant_genes, header=None)
                drug_map = zip(df[0], df[1])
                drugs_resistant_genes = set()
                for item in drug_map:
                    drug = item[0]
                    gene = item[1]
                    if gene in all_genes:
                        drugs_resistant_genes.add(drug)
                drugs_resistant_genes = list(drugs_resistant_genes)
                drugs_resistant_genes.sort()

                summary_file.write(f"Drugs the affected genes are responsive to: "
                f"{', '.join(drugs_responsive_genes) if len(drugs_responsive_genes) else 'no associated drugs were found'}.\n\n")
                summary_file.write(f"Drugs the affected genes are resistant to: "
                f"{', '.join(drugs_resistant_genes) if len(drugs_resistant_genes) else 'no associated drugs were found'}.\n")

rule create_derived_checksum:
    params:
        sym_derived_path = sym_derived_path,
        analysis_prefix = analysis_prefix
    input:
        derived_symlinks = [os.path.join(sym_derived_path, f"{analysis_prefix}__{filename}") for filename in derived_file_names]
    output:
        checksum_file = os.path.join(sym_derived_path, f"{analysis_prefix}__derived_files.md5")
    shell:
        "cd {params.sym_derived_path}; find . -exec md5sum '{{}}' \; >  {output.checksum_file}"
